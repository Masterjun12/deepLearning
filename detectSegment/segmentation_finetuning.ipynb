{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoCodeProgram/deepLearning/blob/main/detectSegment/segmentation_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRuDWURo6lZ2",
        "outputId": "5b51adfa-f541-457b-89e8-05f55c3b6038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "if torch.backends.mps.is_available():\n",
        "    my_device = torch.device('mps')\n",
        "elif torch.cuda.is_available():\n",
        "    my_device = torch.device('cuda')\n",
        "else:\n",
        "    my_device = torch.device('cpu')\n",
        "print(my_device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5_E2LcL6sUU",
        "outputId": "49aa928e-4ee2-4f08-e796-fa4aa042c5bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.segmentation import deeplabv3_mobilenet_v3_large, DeepLabV3_MobileNet_V3_Large_Weights\n",
        "\n",
        "weights = DeepLabV3_MobileNet_V3_Large_Weights.DEFAULT\n",
        "model = deeplabv3_mobilenet_v3_large(weights=weights)"
      ],
      "metadata": {
        "id": "_3m270c_62Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.draw import random_shapes, rectangle\n",
        "\n",
        "def generate_segmentation_rect(image_size):\n",
        "    image, labels = random_shapes(image_size, max_shapes=10, min_shapes=5,  allow_overlap=False)\n",
        "    image = 255 - image\n",
        "\n",
        "    mask = np.zeros(image.shape[:2], dtype=float)\n",
        "    for shape, ((start_row, end_row), (start_col, end_col)) in labels:\n",
        "        if shape == \"rectangle\":\n",
        "            # print(shape, ((start_row, start_col), (end_row, end_col)) )\n",
        "            rr, cc = rectangle(start=(start_row, start_col),\n",
        "                                    end=(end_row, end_col),\n",
        "                                    shape=image_size)\n",
        "            mask[rr, cc] = 1.0\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "image_size = (512, 512)\n",
        "image, mask = generate_segmentation_rect(image_size)\n",
        "print(mask.shape)\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "ax[0].imshow(image)\n",
        "ax[0].set_title('Random Shapes')\n",
        "ax[0].axis('off')\n",
        "\n",
        "ax[1].imshow(mask, cmap='gray')\n",
        "ax[1].set_title('Mask for Rectangle')\n",
        "ax[1].axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "GWNho-qz63Py",
        "outputId": "c49c9aba-81a2-493a-b532-c323ddca15da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(512, 512)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "in_channels = model.classifier[-1].in_channels\n",
        "aux_in_channels = model.aux_classifier[-1].in_channels\n",
        "\n",
        "model.classifier[-1] = nn.Conv2d(\n",
        "    in_channels, num_classes, kernel_size=(1, 1), stride=(1, 1)\n",
        ")\n",
        "model.aux_classifier[-1] = nn.Conv2d(\n",
        "    aux_in_channels, num_classes, kernel_size=(1, 1), stride=(1, 1)\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
      ],
      "metadata": {
        "id": "rjzRWwdN65Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "def get_batch_tensor(batch_size=8):\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    # Generate batch of images and labels\n",
        "    for _ in range(batch_size):\n",
        "        image, label = generate_segmentation_rect(image_size)\n",
        "\n",
        "        tensor_image = TF.to_tensor(image)\n",
        "        normalized_image = normalize(tensor_image)\n",
        "        tensor_label = torch.from_numpy(label).long()\n",
        "\n",
        "\n",
        "        images.append(normalized_image)\n",
        "        labels.append(tensor_label)\n",
        "\n",
        "    batch_images = torch.stack(images)\n",
        "    batch_labels = torch.stack(labels)\n",
        "\n",
        "    return batch_images, batch_labels\n",
        "\n",
        "batch_images, batch_labels = get_batch_tensor(8)\n"
      ],
      "metadata": {
        "id": "JLVRHY3166ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(my_device)\n",
        "\n",
        "model.train()\n",
        "for batch_idx in range(1000):\n",
        "    images, labels = get_batch_tensor(64)\n",
        "    images, labels = images.to(my_device), labels.to(my_device)\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs[\"out\"], labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"batch num : {batch_idx}, loss:{loss.item()}\")\n",
        "\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lh8bieZ-69MC",
        "outputId": "aac38e6d-360c-43e5-8534-ae4d10ae58c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch num : 0, loss:0.6241456866264343\n",
            "batch num : 1, loss:0.5967822074890137\n",
            "batch num : 2, loss:0.5641090869903564\n",
            "batch num : 3, loss:0.5492160320281982\n",
            "batch num : 4, loss:0.5190452337265015\n",
            "batch num : 5, loss:0.5122831463813782\n",
            "batch num : 6, loss:0.4927183985710144\n",
            "batch num : 7, loss:0.46422845125198364\n",
            "batch num : 8, loss:0.46961262822151184\n",
            "batch num : 9, loss:0.4409545361995697\n",
            "batch num : 10, loss:0.42897263169288635\n",
            "batch num : 11, loss:0.4027811586856842\n",
            "batch num : 12, loss:0.3883732259273529\n",
            "batch num : 13, loss:0.3482864201068878\n",
            "batch num : 14, loss:0.32845836877822876\n",
            "batch num : 15, loss:0.32203391194343567\n",
            "batch num : 16, loss:0.3083423674106598\n",
            "batch num : 17, loss:0.2966112196445465\n",
            "batch num : 18, loss:0.29644349217414856\n",
            "batch num : 19, loss:0.25976693630218506\n",
            "batch num : 20, loss:0.22436398267745972\n",
            "batch num : 21, loss:0.21086685359477997\n",
            "batch num : 22, loss:0.19187688827514648\n",
            "batch num : 23, loss:0.19597934186458588\n",
            "batch num : 24, loss:0.17608420550823212\n",
            "batch num : 25, loss:0.16771875321865082\n",
            "batch num : 26, loss:0.1594557762145996\n",
            "batch num : 27, loss:0.16468782722949982\n",
            "batch num : 28, loss:0.17954690754413605\n",
            "batch num : 29, loss:0.1463737189769745\n",
            "batch num : 30, loss:0.14759281277656555\n",
            "batch num : 31, loss:0.13523416221141815\n",
            "batch num : 32, loss:0.12814761698246002\n",
            "batch num : 33, loss:0.12161819636821747\n",
            "batch num : 34, loss:0.11480662226676941\n",
            "batch num : 35, loss:0.12524545192718506\n",
            "batch num : 36, loss:0.12634281814098358\n",
            "batch num : 37, loss:0.10605324804782867\n",
            "batch num : 38, loss:0.10355126112699509\n",
            "batch num : 39, loss:0.10013213008642197\n",
            "batch num : 40, loss:0.10145941376686096\n",
            "batch num : 41, loss:0.09543465822935104\n",
            "batch num : 42, loss:0.09258262068033218\n",
            "batch num : 43, loss:0.09391903877258301\n",
            "batch num : 44, loss:0.09135450422763824\n",
            "batch num : 45, loss:0.08523785322904587\n",
            "batch num : 46, loss:0.08783534169197083\n",
            "batch num : 47, loss:0.08098787069320679\n",
            "batch num : 48, loss:0.08258864283561707\n",
            "batch num : 49, loss:0.07925309240818024\n",
            "batch num : 50, loss:0.0807083249092102\n",
            "batch num : 51, loss:0.08674561232328415\n",
            "batch num : 52, loss:0.07629638910293579\n",
            "batch num : 53, loss:0.07927969843149185\n",
            "batch num : 54, loss:0.07364390790462494\n",
            "batch num : 55, loss:0.07399433851242065\n",
            "batch num : 56, loss:0.07599816471338272\n",
            "batch num : 57, loss:0.07662668079137802\n",
            "batch num : 58, loss:0.07586348056793213\n",
            "batch num : 59, loss:0.07386700809001923\n",
            "batch num : 60, loss:0.07113044708967209\n",
            "batch num : 61, loss:0.07801572233438492\n",
            "batch num : 62, loss:0.07266709208488464\n",
            "batch num : 63, loss:0.0699264258146286\n",
            "batch num : 64, loss:0.0650382936000824\n",
            "batch num : 65, loss:0.07555926591157913\n",
            "batch num : 66, loss:0.06592720001935959\n",
            "batch num : 67, loss:0.06634998321533203\n",
            "batch num : 68, loss:0.06992756575345993\n",
            "batch num : 69, loss:0.06635153293609619\n",
            "batch num : 70, loss:0.07804550975561142\n",
            "batch num : 71, loss:0.06951326131820679\n",
            "batch num : 72, loss:0.06245732307434082\n",
            "batch num : 73, loss:0.06339257955551147\n",
            "batch num : 74, loss:0.060430221259593964\n",
            "batch num : 75, loss:0.06455442309379578\n",
            "batch num : 76, loss:0.06622239947319031\n",
            "batch num : 77, loss:0.062048912048339844\n",
            "batch num : 78, loss:0.05773268640041351\n",
            "batch num : 79, loss:0.06935156136751175\n",
            "batch num : 80, loss:0.06055571138858795\n",
            "batch num : 81, loss:0.05853424593806267\n",
            "batch num : 82, loss:0.05994709953665733\n",
            "batch num : 83, loss:0.05901101976633072\n",
            "batch num : 84, loss:0.055979885160923004\n",
            "batch num : 85, loss:0.05946233868598938\n",
            "batch num : 86, loss:0.05928409844636917\n",
            "batch num : 87, loss:0.05862801522016525\n",
            "batch num : 88, loss:0.05676241219043732\n",
            "batch num : 89, loss:0.054364562034606934\n",
            "batch num : 90, loss:0.054139330983161926\n",
            "batch num : 91, loss:0.05703166127204895\n",
            "batch num : 92, loss:0.054311562329530716\n",
            "batch num : 93, loss:0.06073615327477455\n",
            "batch num : 94, loss:0.05614914000034332\n",
            "batch num : 95, loss:0.05433911085128784\n",
            "batch num : 96, loss:0.05696088448166847\n",
            "batch num : 97, loss:0.053164005279541016\n",
            "batch num : 98, loss:0.059053145349025726\n",
            "batch num : 99, loss:0.050787389278411865\n",
            "batch num : 100, loss:0.05511385202407837\n",
            "batch num : 101, loss:0.0503663532435894\n",
            "batch num : 102, loss:0.0506509505212307\n",
            "batch num : 103, loss:0.05106409639120102\n",
            "batch num : 104, loss:0.0499105341732502\n",
            "batch num : 105, loss:0.048743702471256256\n",
            "batch num : 106, loss:0.05177183449268341\n",
            "batch num : 107, loss:0.05186077579855919\n",
            "batch num : 108, loss:0.05369619280099869\n",
            "batch num : 109, loss:0.0512826107442379\n",
            "batch num : 110, loss:0.05352197587490082\n",
            "batch num : 111, loss:0.04748924821615219\n",
            "batch num : 112, loss:0.04985220730304718\n",
            "batch num : 113, loss:0.04832083359360695\n",
            "batch num : 114, loss:0.04983213171362877\n",
            "batch num : 115, loss:0.05361640080809593\n",
            "batch num : 116, loss:0.04999862238764763\n",
            "batch num : 117, loss:0.04636332020163536\n",
            "batch num : 118, loss:0.050594672560691833\n",
            "batch num : 119, loss:0.04899423196911812\n",
            "batch num : 120, loss:0.046463336795568466\n",
            "batch num : 121, loss:0.046773869544267654\n",
            "batch num : 122, loss:0.04685990512371063\n",
            "batch num : 123, loss:0.045502569526433945\n",
            "batch num : 124, loss:0.0465242974460125\n",
            "batch num : 125, loss:0.044713445007801056\n",
            "batch num : 126, loss:0.04335519298911095\n",
            "batch num : 127, loss:0.04402340576052666\n",
            "batch num : 128, loss:0.04269659146666527\n",
            "batch num : 129, loss:0.04633523151278496\n",
            "batch num : 130, loss:0.043547455221414566\n",
            "batch num : 131, loss:0.04262300953269005\n",
            "batch num : 132, loss:0.044702887535095215\n",
            "batch num : 133, loss:0.04236065596342087\n",
            "batch num : 134, loss:0.04262274131178856\n",
            "batch num : 135, loss:0.04188980162143707\n",
            "batch num : 136, loss:0.04721660912036896\n",
            "batch num : 137, loss:0.0425887331366539\n",
            "batch num : 138, loss:0.04508202150464058\n",
            "batch num : 139, loss:0.04402785003185272\n",
            "batch num : 140, loss:0.042729876935482025\n",
            "batch num : 141, loss:0.04237097501754761\n",
            "batch num : 142, loss:0.04473375901579857\n",
            "batch num : 143, loss:0.04447900503873825\n",
            "batch num : 144, loss:0.042886070907115936\n",
            "batch num : 145, loss:0.040927547961473465\n",
            "batch num : 146, loss:0.040182046592235565\n",
            "batch num : 147, loss:0.04199839383363724\n",
            "batch num : 148, loss:0.04027897119522095\n",
            "batch num : 149, loss:0.03733144700527191\n",
            "batch num : 150, loss:0.040223512798547745\n",
            "batch num : 151, loss:0.040334977209568024\n",
            "batch num : 152, loss:0.03995775431394577\n",
            "batch num : 153, loss:0.04048462212085724\n",
            "batch num : 154, loss:0.041488226503133774\n",
            "batch num : 155, loss:0.03907803073525429\n",
            "batch num : 156, loss:0.03873904421925545\n",
            "batch num : 157, loss:0.042045708745718\n",
            "batch num : 158, loss:0.03931013494729996\n",
            "batch num : 159, loss:0.038095928728580475\n",
            "batch num : 160, loss:0.03809972479939461\n",
            "batch num : 161, loss:0.038475897163152695\n",
            "batch num : 162, loss:0.037427868694067\n",
            "batch num : 163, loss:0.03867756202816963\n",
            "batch num : 164, loss:0.03747038170695305\n",
            "batch num : 165, loss:0.03787638619542122\n",
            "batch num : 166, loss:0.039516448974609375\n",
            "batch num : 167, loss:0.036948829889297485\n",
            "batch num : 168, loss:0.0403810478746891\n",
            "batch num : 169, loss:0.03751368075609207\n",
            "batch num : 170, loss:0.036816228181123734\n",
            "batch num : 171, loss:0.037456054240465164\n",
            "batch num : 172, loss:0.03910958021879196\n",
            "batch num : 173, loss:0.03798257187008858\n",
            "batch num : 174, loss:0.037668611854314804\n",
            "batch num : 175, loss:0.03959057852625847\n",
            "batch num : 176, loss:0.03538907691836357\n",
            "batch num : 177, loss:0.03771499544382095\n",
            "batch num : 178, loss:0.03524037078022957\n",
            "batch num : 179, loss:0.03548290207982063\n",
            "batch num : 180, loss:0.03606610372662544\n",
            "batch num : 181, loss:0.03423047810792923\n",
            "batch num : 182, loss:0.0355144664645195\n",
            "batch num : 183, loss:0.03498252108693123\n",
            "batch num : 184, loss:0.036974627524614334\n",
            "batch num : 185, loss:0.03432995826005936\n",
            "batch num : 186, loss:0.03464444726705551\n",
            "batch num : 187, loss:0.032803330570459366\n",
            "batch num : 188, loss:0.034865718334913254\n",
            "batch num : 189, loss:0.03438611701130867\n",
            "batch num : 190, loss:0.03278825432062149\n",
            "batch num : 191, loss:0.03316782787442207\n",
            "batch num : 192, loss:0.03384010121226311\n",
            "batch num : 193, loss:0.03457115218043327\n",
            "batch num : 194, loss:0.03329027444124222\n",
            "batch num : 195, loss:0.03369631990790367\n",
            "batch num : 196, loss:0.03489815071225166\n",
            "batch num : 197, loss:0.03157120570540428\n",
            "batch num : 198, loss:0.037753667682409286\n",
            "batch num : 199, loss:0.035704195499420166\n",
            "batch num : 200, loss:0.03298850730061531\n",
            "batch num : 201, loss:0.034218065440654755\n",
            "batch num : 202, loss:0.03163634240627289\n",
            "batch num : 203, loss:0.034127045422792435\n",
            "batch num : 204, loss:0.034288931638002396\n",
            "batch num : 205, loss:0.032847631722688675\n",
            "batch num : 206, loss:0.031121715903282166\n",
            "batch num : 207, loss:0.03511171042919159\n",
            "batch num : 208, loss:0.03150949999690056\n",
            "batch num : 209, loss:0.031168807297945023\n",
            "batch num : 210, loss:0.03208998590707779\n",
            "batch num : 211, loss:0.03474583476781845\n",
            "batch num : 212, loss:0.03019319474697113\n",
            "batch num : 213, loss:0.03135520592331886\n",
            "batch num : 214, loss:0.030161770060658455\n",
            "batch num : 215, loss:0.03309163078665733\n",
            "batch num : 216, loss:0.02889217622578144\n",
            "batch num : 217, loss:0.030748574063181877\n",
            "batch num : 218, loss:0.030493993312120438\n",
            "batch num : 219, loss:0.02895621582865715\n",
            "batch num : 220, loss:0.03030555136501789\n",
            "batch num : 221, loss:0.029965978115797043\n",
            "batch num : 222, loss:0.029928280040621758\n",
            "batch num : 223, loss:0.029335279017686844\n",
            "batch num : 224, loss:0.031493280082941055\n",
            "batch num : 225, loss:0.029596636071801186\n",
            "batch num : 226, loss:0.02929781749844551\n",
            "batch num : 227, loss:0.02938254550099373\n",
            "batch num : 228, loss:0.03139489144086838\n",
            "batch num : 229, loss:0.028726726770401\n",
            "batch num : 230, loss:0.029578257352113724\n",
            "batch num : 231, loss:0.028727766126394272\n",
            "batch num : 232, loss:0.03140801191329956\n",
            "batch num : 233, loss:0.028707118704915047\n",
            "batch num : 234, loss:0.029819749295711517\n",
            "batch num : 235, loss:0.03141635283827782\n",
            "batch num : 236, loss:0.03182505443692207\n",
            "batch num : 237, loss:0.02833475172519684\n",
            "batch num : 238, loss:0.029116131365299225\n",
            "batch num : 239, loss:0.03000319004058838\n",
            "batch num : 240, loss:0.029169267043471336\n",
            "batch num : 241, loss:0.02814592979848385\n",
            "batch num : 242, loss:0.02714981511235237\n",
            "batch num : 243, loss:0.028829501941800117\n",
            "batch num : 244, loss:0.029904037714004517\n",
            "batch num : 245, loss:0.028385523706674576\n",
            "batch num : 246, loss:0.027586879208683968\n",
            "batch num : 247, loss:0.028360780328512192\n",
            "batch num : 248, loss:0.028676845133304596\n",
            "batch num : 249, loss:0.02839168719947338\n",
            "batch num : 250, loss:0.02620600536465645\n",
            "batch num : 251, loss:0.02809406816959381\n",
            "batch num : 252, loss:0.026038890704512596\n",
            "batch num : 253, loss:0.02659057080745697\n",
            "batch num : 254, loss:0.028851361945271492\n",
            "batch num : 255, loss:0.027825597673654556\n",
            "batch num : 256, loss:0.026460519060492516\n",
            "batch num : 257, loss:0.026613712310791016\n",
            "batch num : 258, loss:0.02427629381418228\n",
            "batch num : 259, loss:0.025653816759586334\n",
            "batch num : 260, loss:0.025971217080950737\n",
            "batch num : 261, loss:0.026750104501843452\n",
            "batch num : 262, loss:0.026789484545588493\n",
            "batch num : 263, loss:0.027711326256394386\n",
            "batch num : 264, loss:0.026450704783201218\n",
            "batch num : 265, loss:0.027056097984313965\n",
            "batch num : 266, loss:0.02512296847999096\n",
            "batch num : 267, loss:0.028430474922060966\n",
            "batch num : 268, loss:0.022912612184882164\n",
            "batch num : 269, loss:0.02781705930829048\n",
            "batch num : 270, loss:0.027474544942378998\n",
            "batch num : 271, loss:0.027112945914268494\n",
            "batch num : 272, loss:0.026944158598780632\n",
            "batch num : 273, loss:0.026232752948999405\n",
            "batch num : 274, loss:0.02535228617489338\n",
            "batch num : 275, loss:0.02685672789812088\n",
            "batch num : 276, loss:0.02588295191526413\n",
            "batch num : 277, loss:0.026385651901364326\n",
            "batch num : 278, loss:0.024414101615548134\n",
            "batch num : 279, loss:0.026209719479084015\n",
            "batch num : 280, loss:0.024174902588129044\n",
            "batch num : 281, loss:0.02602175623178482\n",
            "batch num : 282, loss:0.02536262944340706\n",
            "batch num : 283, loss:0.027807461097836494\n",
            "batch num : 284, loss:0.0250321663916111\n",
            "batch num : 285, loss:0.0272783525288105\n",
            "batch num : 286, loss:0.02601463347673416\n",
            "batch num : 287, loss:0.027220968157052994\n",
            "batch num : 288, loss:0.024361416697502136\n",
            "batch num : 289, loss:0.0229411069303751\n",
            "batch num : 290, loss:0.02328597567975521\n",
            "batch num : 291, loss:0.025820815935730934\n",
            "batch num : 292, loss:0.024042783305048943\n",
            "batch num : 293, loss:0.02482684515416622\n",
            "batch num : 294, loss:0.024293681606650352\n",
            "batch num : 295, loss:0.025324704125523567\n",
            "batch num : 296, loss:0.023222148418426514\n",
            "batch num : 297, loss:0.02439858205616474\n",
            "batch num : 298, loss:0.02388579584658146\n",
            "batch num : 299, loss:0.024612385779619217\n",
            "batch num : 300, loss:0.022725140675902367\n",
            "batch num : 301, loss:0.022620903328061104\n",
            "batch num : 302, loss:0.02417105622589588\n",
            "batch num : 303, loss:0.022731440141797066\n",
            "batch num : 304, loss:0.024852510541677475\n",
            "batch num : 305, loss:0.023170826956629753\n",
            "batch num : 306, loss:0.02593112736940384\n",
            "batch num : 307, loss:0.023247569799423218\n",
            "batch num : 308, loss:0.02404545620083809\n",
            "batch num : 309, loss:0.02301035262644291\n",
            "batch num : 310, loss:0.022539610043168068\n",
            "batch num : 311, loss:0.020657731220126152\n",
            "batch num : 312, loss:0.022508397698402405\n",
            "batch num : 313, loss:0.022472957149147987\n",
            "batch num : 314, loss:0.024465834721922874\n",
            "batch num : 315, loss:0.022157473489642143\n",
            "batch num : 316, loss:0.022480404004454613\n",
            "batch num : 317, loss:0.02352147549390793\n",
            "batch num : 318, loss:0.02326401323080063\n",
            "batch num : 319, loss:0.023775188252329826\n",
            "batch num : 320, loss:0.022212684154510498\n",
            "batch num : 321, loss:0.023139700293540955\n",
            "batch num : 322, loss:0.022552037611603737\n",
            "batch num : 323, loss:0.021472470834851265\n",
            "batch num : 324, loss:0.024285482242703438\n",
            "batch num : 325, loss:0.020850198343396187\n",
            "batch num : 326, loss:0.023552868515253067\n",
            "batch num : 327, loss:0.02258153446018696\n",
            "batch num : 328, loss:0.023038167506456375\n",
            "batch num : 329, loss:0.022424623370170593\n",
            "batch num : 330, loss:0.02271055057644844\n",
            "batch num : 331, loss:0.022313637658953667\n",
            "batch num : 332, loss:0.022157687693834305\n",
            "batch num : 333, loss:0.022950388491153717\n",
            "batch num : 334, loss:0.022440681234002113\n",
            "batch num : 335, loss:0.021860357373952866\n",
            "batch num : 336, loss:0.02173100970685482\n",
            "batch num : 337, loss:0.022881127893924713\n",
            "batch num : 338, loss:0.021778620779514313\n",
            "batch num : 339, loss:0.02145453728735447\n",
            "batch num : 340, loss:0.021561969071626663\n",
            "batch num : 341, loss:0.02051083743572235\n",
            "batch num : 342, loss:0.018892010673880577\n",
            "batch num : 343, loss:0.021225659176707268\n",
            "batch num : 344, loss:0.020450027659535408\n",
            "batch num : 345, loss:0.019909057766199112\n",
            "batch num : 346, loss:0.02266041934490204\n",
            "batch num : 347, loss:0.022277599200606346\n",
            "batch num : 348, loss:0.02168559655547142\n",
            "batch num : 349, loss:0.021024469286203384\n",
            "batch num : 350, loss:0.02199547179043293\n",
            "batch num : 351, loss:0.01926116645336151\n",
            "batch num : 352, loss:0.02188669890165329\n",
            "batch num : 353, loss:0.020761840045452118\n",
            "batch num : 354, loss:0.02143542468547821\n",
            "batch num : 355, loss:0.020625844597816467\n",
            "batch num : 356, loss:0.02272850275039673\n",
            "batch num : 357, loss:0.02219468168914318\n",
            "batch num : 358, loss:0.020646918565034866\n",
            "batch num : 359, loss:0.021469779312610626\n",
            "batch num : 360, loss:0.021501218900084496\n",
            "batch num : 361, loss:0.0220742616802454\n",
            "batch num : 362, loss:0.020557912066578865\n",
            "batch num : 363, loss:0.020703280344605446\n",
            "batch num : 364, loss:0.020076746121048927\n",
            "batch num : 365, loss:0.020362572744488716\n",
            "batch num : 366, loss:0.020195916295051575\n",
            "batch num : 367, loss:0.020471271127462387\n",
            "batch num : 368, loss:0.02104356326162815\n",
            "batch num : 369, loss:0.020576326176524162\n",
            "batch num : 370, loss:0.019341813400387764\n",
            "batch num : 371, loss:0.02079707942903042\n",
            "batch num : 372, loss:0.0208150502294302\n",
            "batch num : 373, loss:0.01982862316071987\n",
            "batch num : 374, loss:0.019891951233148575\n",
            "batch num : 375, loss:0.020071284845471382\n",
            "batch num : 376, loss:0.02142416685819626\n",
            "batch num : 377, loss:0.020770080387592316\n",
            "batch num : 378, loss:0.01975138485431671\n",
            "batch num : 379, loss:0.019570590928196907\n",
            "batch num : 380, loss:0.02094735950231552\n",
            "batch num : 381, loss:0.01988259144127369\n",
            "batch num : 382, loss:0.02054774761199951\n",
            "batch num : 383, loss:0.018628332763910294\n",
            "batch num : 384, loss:0.018930550664663315\n",
            "batch num : 385, loss:0.02007864974439144\n",
            "batch num : 386, loss:0.01963655650615692\n",
            "batch num : 387, loss:0.018360435962677002\n",
            "batch num : 388, loss:0.01819293014705181\n",
            "batch num : 389, loss:0.0192965529859066\n",
            "batch num : 390, loss:0.019335901364684105\n",
            "batch num : 391, loss:0.018154442310333252\n",
            "batch num : 392, loss:0.01841241866350174\n",
            "batch num : 393, loss:0.01937633939087391\n",
            "batch num : 394, loss:0.018012775108218193\n",
            "batch num : 395, loss:0.02123701199889183\n",
            "batch num : 396, loss:0.018954256549477577\n",
            "batch num : 397, loss:0.019453374668955803\n",
            "batch num : 398, loss:0.018409477546811104\n",
            "batch num : 399, loss:0.01914142817258835\n",
            "batch num : 400, loss:0.019710222259163857\n",
            "batch num : 401, loss:0.01813078112900257\n",
            "batch num : 402, loss:0.018419891595840454\n",
            "batch num : 403, loss:0.020345302298665047\n",
            "batch num : 404, loss:0.0182529017329216\n",
            "batch num : 405, loss:0.019156578928232193\n",
            "batch num : 406, loss:0.01926286332309246\n",
            "batch num : 407, loss:0.018047194927930832\n",
            "batch num : 408, loss:0.01800999976694584\n",
            "batch num : 409, loss:0.01746901124715805\n",
            "batch num : 410, loss:0.01796821504831314\n",
            "batch num : 411, loss:0.017007924616336823\n",
            "batch num : 412, loss:0.02004004456102848\n",
            "batch num : 413, loss:0.017881399020552635\n",
            "batch num : 414, loss:0.019218074157834053\n",
            "batch num : 415, loss:0.01759839989244938\n",
            "batch num : 416, loss:0.019197456538677216\n",
            "batch num : 417, loss:0.017885146662592888\n",
            "batch num : 418, loss:0.016597673296928406\n",
            "batch num : 419, loss:0.018284805119037628\n",
            "batch num : 420, loss:0.01753789745271206\n",
            "batch num : 421, loss:0.017870692536234856\n",
            "batch num : 422, loss:0.01910998299717903\n",
            "batch num : 423, loss:0.01888367347419262\n",
            "batch num : 424, loss:0.01959337294101715\n",
            "batch num : 425, loss:0.018059367313981056\n",
            "batch num : 426, loss:0.01800735481083393\n",
            "batch num : 427, loss:0.01849530078470707\n",
            "batch num : 428, loss:0.01865357533097267\n",
            "batch num : 429, loss:0.01770871691405773\n",
            "batch num : 430, loss:0.018311217427253723\n",
            "batch num : 431, loss:0.018770698457956314\n",
            "batch num : 432, loss:0.018062438815832138\n",
            "batch num : 433, loss:0.01766894944012165\n",
            "batch num : 434, loss:0.01805058680474758\n",
            "batch num : 435, loss:0.017464088276028633\n",
            "batch num : 436, loss:0.01711154356598854\n",
            "batch num : 437, loss:0.017375396564602852\n",
            "batch num : 438, loss:0.017628150060772896\n",
            "batch num : 439, loss:0.01902753673493862\n",
            "batch num : 440, loss:0.017046485096216202\n",
            "batch num : 441, loss:0.01796942576766014\n",
            "batch num : 442, loss:0.01739048957824707\n",
            "batch num : 443, loss:0.01652105525135994\n",
            "batch num : 444, loss:0.0165328960865736\n",
            "batch num : 445, loss:0.017779795452952385\n",
            "batch num : 446, loss:0.016770411282777786\n",
            "batch num : 447, loss:0.0187413077801466\n",
            "batch num : 448, loss:0.01617676205933094\n",
            "batch num : 449, loss:0.015424561686813831\n",
            "batch num : 450, loss:0.0188321340829134\n",
            "batch num : 451, loss:0.017790088430047035\n",
            "batch num : 452, loss:0.017127104103565216\n",
            "batch num : 453, loss:0.016993749886751175\n",
            "batch num : 454, loss:0.017477260902523994\n",
            "batch num : 455, loss:0.018195519223809242\n",
            "batch num : 456, loss:0.018562735989689827\n",
            "batch num : 457, loss:0.017591457813978195\n",
            "batch num : 458, loss:0.015552249737083912\n",
            "batch num : 459, loss:0.01670548878610134\n",
            "batch num : 460, loss:0.01681732013821602\n",
            "batch num : 461, loss:0.017294825986027718\n",
            "batch num : 462, loss:0.016374943777918816\n",
            "batch num : 463, loss:0.01708163321018219\n",
            "batch num : 464, loss:0.017558500170707703\n",
            "batch num : 465, loss:0.016498342156410217\n",
            "batch num : 466, loss:0.016102977097034454\n",
            "batch num : 467, loss:0.016839032992720604\n",
            "batch num : 468, loss:0.018934743478894234\n",
            "batch num : 469, loss:0.018025225028395653\n",
            "batch num : 470, loss:0.016965243965387344\n",
            "batch num : 471, loss:0.01678563468158245\n",
            "batch num : 472, loss:0.01625777967274189\n",
            "batch num : 473, loss:0.01678924262523651\n",
            "batch num : 474, loss:0.015583046711981297\n",
            "batch num : 475, loss:0.016532065346837044\n",
            "batch num : 476, loss:0.015549627132713795\n",
            "batch num : 477, loss:0.016935192048549652\n",
            "batch num : 478, loss:0.017833679914474487\n",
            "batch num : 479, loss:0.015169023536145687\n",
            "batch num : 480, loss:0.01654009334743023\n",
            "batch num : 481, loss:0.0177433043718338\n",
            "batch num : 482, loss:0.016041267663240433\n",
            "batch num : 483, loss:0.017468346282839775\n",
            "batch num : 484, loss:0.01781432144343853\n",
            "batch num : 485, loss:0.01630815677344799\n",
            "batch num : 486, loss:0.016377095133066177\n",
            "batch num : 487, loss:0.016383182257413864\n",
            "batch num : 488, loss:0.016568677499890327\n",
            "batch num : 489, loss:0.01592080108821392\n",
            "batch num : 490, loss:0.01591523177921772\n",
            "batch num : 491, loss:0.016439106315374374\n",
            "batch num : 492, loss:0.01707456447184086\n",
            "batch num : 493, loss:0.017234262079000473\n",
            "batch num : 494, loss:0.01688218303024769\n",
            "batch num : 495, loss:0.015207844786345959\n",
            "batch num : 496, loss:0.01751893386244774\n",
            "batch num : 497, loss:0.015172263607382774\n",
            "batch num : 498, loss:0.015484659932553768\n",
            "batch num : 499, loss:0.015351793728768826\n",
            "batch num : 500, loss:0.01744741201400757\n",
            "batch num : 501, loss:0.01605761982500553\n",
            "batch num : 502, loss:0.014573691412806511\n",
            "batch num : 503, loss:0.01511087641119957\n",
            "batch num : 504, loss:0.017001256346702576\n",
            "batch num : 505, loss:0.015608971007168293\n",
            "batch num : 506, loss:0.015581551939249039\n",
            "batch num : 507, loss:0.0162318367511034\n",
            "batch num : 508, loss:0.014391930773854256\n",
            "batch num : 509, loss:0.015457014553248882\n",
            "batch num : 510, loss:0.01581486128270626\n",
            "batch num : 511, loss:0.01619580015540123\n",
            "batch num : 512, loss:0.014554949477314949\n",
            "batch num : 513, loss:0.01614190638065338\n",
            "batch num : 514, loss:0.016281751915812492\n",
            "batch num : 515, loss:0.015648653730750084\n",
            "batch num : 516, loss:0.016487041488289833\n",
            "batch num : 517, loss:0.014445680193603039\n",
            "batch num : 518, loss:0.015604356303811073\n",
            "batch num : 519, loss:0.015123468823730946\n",
            "batch num : 520, loss:0.014199797064065933\n",
            "batch num : 521, loss:0.013735673390328884\n",
            "batch num : 522, loss:0.017345745116472244\n",
            "batch num : 523, loss:0.015366104431450367\n",
            "batch num : 524, loss:0.015219304710626602\n",
            "batch num : 525, loss:0.015674710273742676\n",
            "batch num : 526, loss:0.01577228121459484\n",
            "batch num : 527, loss:0.014493624679744244\n",
            "batch num : 528, loss:0.015360957942903042\n",
            "batch num : 529, loss:0.015543821267783642\n",
            "batch num : 530, loss:0.014075811952352524\n",
            "batch num : 531, loss:0.014406266622245312\n",
            "batch num : 532, loss:0.01427189540117979\n",
            "batch num : 533, loss:0.01546482089906931\n",
            "batch num : 534, loss:0.016694296151399612\n",
            "batch num : 535, loss:0.016117850318551064\n",
            "batch num : 536, loss:0.015058930963277817\n",
            "batch num : 537, loss:0.01631806045770645\n",
            "batch num : 538, loss:0.014681532979011536\n",
            "batch num : 539, loss:0.015760954469442368\n",
            "batch num : 540, loss:0.014829281717538834\n",
            "batch num : 541, loss:0.013836365193128586\n",
            "batch num : 542, loss:0.015062819235026836\n",
            "batch num : 543, loss:0.015850376337766647\n",
            "batch num : 544, loss:0.014625750482082367\n",
            "batch num : 545, loss:0.015716707333922386\n",
            "batch num : 546, loss:0.014427310787141323\n",
            "batch num : 547, loss:0.015192483551800251\n",
            "batch num : 548, loss:0.013642201200127602\n",
            "batch num : 549, loss:0.014144495129585266\n",
            "batch num : 550, loss:0.01632162742316723\n",
            "batch num : 551, loss:0.015047101303935051\n",
            "batch num : 552, loss:0.013992720283567905\n",
            "batch num : 553, loss:0.014453153125941753\n",
            "batch num : 554, loss:0.01452067494392395\n",
            "batch num : 555, loss:0.015105342492461205\n",
            "batch num : 556, loss:0.014167040586471558\n",
            "batch num : 557, loss:0.017295239493250847\n",
            "batch num : 558, loss:0.0158601813018322\n",
            "batch num : 559, loss:0.013907462358474731\n",
            "batch num : 560, loss:0.014552807435393333\n",
            "batch num : 561, loss:0.014038541354238987\n",
            "batch num : 562, loss:0.013791406527161598\n",
            "batch num : 563, loss:0.01497083529829979\n",
            "batch num : 564, loss:0.014317342080175877\n",
            "batch num : 565, loss:0.013783485628664494\n",
            "batch num : 566, loss:0.012890764512121677\n",
            "batch num : 567, loss:0.013829878531396389\n",
            "batch num : 568, loss:0.013918468728661537\n",
            "batch num : 569, loss:0.014263332821428776\n",
            "batch num : 570, loss:0.015799719840288162\n",
            "batch num : 571, loss:0.014006884768605232\n",
            "batch num : 572, loss:0.015561934560537338\n",
            "batch num : 573, loss:0.014954841695725918\n",
            "batch num : 574, loss:0.014124239794909954\n",
            "batch num : 575, loss:0.014944805763661861\n",
            "batch num : 576, loss:0.013273680582642555\n",
            "batch num : 577, loss:0.014643952250480652\n",
            "batch num : 578, loss:0.01307794451713562\n",
            "batch num : 579, loss:0.014452848583459854\n",
            "batch num : 580, loss:0.014883332885801792\n",
            "batch num : 581, loss:0.014185843989253044\n",
            "batch num : 582, loss:0.014111333526670933\n",
            "batch num : 583, loss:0.01405339129269123\n",
            "batch num : 584, loss:0.015428908169269562\n",
            "batch num : 585, loss:0.013539324514567852\n",
            "batch num : 586, loss:0.015532981604337692\n",
            "batch num : 587, loss:0.015108444727957249\n",
            "batch num : 588, loss:0.014587601646780968\n",
            "batch num : 589, loss:0.014224919490516186\n",
            "batch num : 590, loss:0.013948093168437481\n",
            "batch num : 591, loss:0.014597912319004536\n",
            "batch num : 592, loss:0.013577315025031567\n",
            "batch num : 593, loss:0.012785178609192371\n",
            "batch num : 594, loss:0.013378364033997059\n",
            "batch num : 595, loss:0.014345378614962101\n",
            "batch num : 596, loss:0.014291118830442429\n",
            "batch num : 597, loss:0.015280192717909813\n",
            "batch num : 598, loss:0.012981122359633446\n",
            "batch num : 599, loss:0.014055706560611725\n",
            "batch num : 600, loss:0.014967103488743305\n",
            "batch num : 601, loss:0.01206478662788868\n",
            "batch num : 602, loss:0.012567165307700634\n",
            "batch num : 603, loss:0.01401183009147644\n",
            "batch num : 604, loss:0.013349535875022411\n",
            "batch num : 605, loss:0.014399383217096329\n",
            "batch num : 606, loss:0.013239608146250248\n",
            "batch num : 607, loss:0.014018447138369083\n",
            "batch num : 608, loss:0.012227860279381275\n",
            "batch num : 609, loss:0.012821557000279427\n",
            "batch num : 610, loss:0.012756315059959888\n",
            "batch num : 611, loss:0.01416358258575201\n",
            "batch num : 612, loss:0.014010097831487656\n",
            "batch num : 613, loss:0.01276325061917305\n",
            "batch num : 614, loss:0.013467958196997643\n",
            "batch num : 615, loss:0.014918755739927292\n",
            "batch num : 616, loss:0.013631617650389671\n",
            "batch num : 617, loss:0.011384754441678524\n",
            "batch num : 618, loss:0.014157124795019627\n",
            "batch num : 619, loss:0.014824534766376019\n",
            "batch num : 620, loss:0.01359543576836586\n",
            "batch num : 621, loss:0.014666860923171043\n",
            "batch num : 622, loss:0.013665932230651379\n",
            "batch num : 623, loss:0.013685737736523151\n",
            "batch num : 624, loss:0.013068530708551407\n",
            "batch num : 625, loss:0.01597633585333824\n",
            "batch num : 626, loss:0.013119095005095005\n",
            "batch num : 627, loss:0.01469443365931511\n",
            "batch num : 628, loss:0.012466971762478352\n",
            "batch num : 629, loss:0.015378974378108978\n",
            "batch num : 630, loss:0.012596165761351585\n",
            "batch num : 631, loss:0.013865518383681774\n",
            "batch num : 632, loss:0.014676139689981937\n",
            "batch num : 633, loss:0.013690125197172165\n",
            "batch num : 634, loss:0.013359971344470978\n",
            "batch num : 635, loss:0.014283077791333199\n",
            "batch num : 636, loss:0.01360517181456089\n",
            "batch num : 637, loss:0.013207079842686653\n",
            "batch num : 638, loss:0.013754354789853096\n",
            "batch num : 639, loss:0.014965907670557499\n",
            "batch num : 640, loss:0.01529808808118105\n",
            "batch num : 641, loss:0.0167487021535635\n",
            "batch num : 642, loss:0.012526851147413254\n",
            "batch num : 643, loss:0.012820197269320488\n",
            "batch num : 644, loss:0.012406151741743088\n",
            "batch num : 645, loss:0.013860505074262619\n",
            "batch num : 646, loss:0.014056345447897911\n",
            "batch num : 647, loss:0.014916742220520973\n",
            "batch num : 648, loss:0.013198419474065304\n",
            "batch num : 649, loss:0.013899392448365688\n",
            "batch num : 650, loss:0.013441178947687149\n",
            "batch num : 651, loss:0.0139308525249362\n",
            "batch num : 652, loss:0.013396897353231907\n",
            "batch num : 653, loss:0.012640942819416523\n",
            "batch num : 654, loss:0.012304626405239105\n",
            "batch num : 655, loss:0.012984571047127247\n",
            "batch num : 656, loss:0.013206417672336102\n",
            "batch num : 657, loss:0.012553611770272255\n",
            "batch num : 658, loss:0.012629065662622452\n",
            "batch num : 659, loss:0.013108151964843273\n",
            "batch num : 660, loss:0.013419663533568382\n",
            "batch num : 661, loss:0.013097157701849937\n",
            "batch num : 662, loss:0.012208392843604088\n",
            "batch num : 663, loss:0.013667860999703407\n",
            "batch num : 664, loss:0.012648047879338264\n",
            "batch num : 665, loss:0.013032321818172932\n",
            "batch num : 666, loss:0.012110766023397446\n",
            "batch num : 667, loss:0.014113754965364933\n",
            "batch num : 668, loss:0.01269331481307745\n",
            "batch num : 669, loss:0.015293098986148834\n",
            "batch num : 670, loss:0.012766323052346706\n",
            "batch num : 671, loss:0.013781005516648293\n",
            "batch num : 672, loss:0.014499858021736145\n",
            "batch num : 673, loss:0.014115155674517155\n",
            "batch num : 674, loss:0.014767445623874664\n",
            "batch num : 675, loss:0.01321280375123024\n",
            "batch num : 676, loss:0.014187363907694817\n",
            "batch num : 677, loss:0.012705259956419468\n",
            "batch num : 678, loss:0.012658790685236454\n",
            "batch num : 679, loss:0.013545509427785873\n",
            "batch num : 680, loss:0.01313338615000248\n",
            "batch num : 681, loss:0.01378640066832304\n",
            "batch num : 682, loss:0.013429347425699234\n",
            "batch num : 683, loss:0.013358724303543568\n",
            "batch num : 684, loss:0.012116574682295322\n",
            "batch num : 685, loss:0.012492877431213856\n",
            "batch num : 686, loss:0.0114509928971529\n",
            "batch num : 687, loss:0.012250484898686409\n",
            "batch num : 688, loss:0.012110233306884766\n",
            "batch num : 689, loss:0.011205866001546383\n",
            "batch num : 690, loss:0.011256943456828594\n",
            "batch num : 691, loss:0.012537993490695953\n",
            "batch num : 692, loss:0.011235382407903671\n",
            "batch num : 693, loss:0.012538990005850792\n",
            "batch num : 694, loss:0.012388043105602264\n",
            "batch num : 695, loss:0.012900900095701218\n",
            "batch num : 696, loss:0.013345515355467796\n",
            "batch num : 697, loss:0.013055837713181973\n",
            "batch num : 698, loss:0.012177184224128723\n",
            "batch num : 699, loss:0.01290447823703289\n",
            "batch num : 700, loss:0.013317100703716278\n",
            "batch num : 701, loss:0.012751169502735138\n",
            "batch num : 702, loss:0.011712121777236462\n",
            "batch num : 703, loss:0.012097601778805256\n",
            "batch num : 704, loss:0.0124707892537117\n",
            "batch num : 705, loss:0.012116489931941032\n",
            "batch num : 706, loss:0.013044916093349457\n",
            "batch num : 707, loss:0.010201143100857735\n",
            "batch num : 708, loss:0.012611416168510914\n",
            "batch num : 709, loss:0.01097908802330494\n",
            "batch num : 710, loss:0.012272062711417675\n",
            "batch num : 711, loss:0.012430338189005852\n",
            "batch num : 712, loss:0.011769736185669899\n",
            "batch num : 713, loss:0.01339993067085743\n",
            "batch num : 714, loss:0.012398282997310162\n",
            "batch num : 715, loss:0.011301703751087189\n",
            "batch num : 716, loss:0.013861311599612236\n",
            "batch num : 717, loss:0.012802660465240479\n",
            "batch num : 718, loss:0.013801945373415947\n",
            "batch num : 719, loss:0.012795696035027504\n",
            "batch num : 720, loss:0.012651868164539337\n",
            "batch num : 721, loss:0.011886179447174072\n",
            "batch num : 722, loss:0.012089626863598824\n",
            "batch num : 723, loss:0.012882349081337452\n",
            "batch num : 724, loss:0.01277869287878275\n",
            "batch num : 725, loss:0.010871782898902893\n",
            "batch num : 726, loss:0.01216132752597332\n",
            "batch num : 727, loss:0.01399405486881733\n",
            "batch num : 728, loss:0.013096285052597523\n",
            "batch num : 729, loss:0.011044872924685478\n",
            "batch num : 730, loss:0.011346154846251011\n",
            "batch num : 731, loss:0.012923350557684898\n",
            "batch num : 732, loss:0.012039288878440857\n",
            "batch num : 733, loss:0.012271428480744362\n",
            "batch num : 734, loss:0.010851030237972736\n",
            "batch num : 735, loss:0.011591915041208267\n",
            "batch num : 736, loss:0.011186509393155575\n",
            "batch num : 737, loss:0.01074991375207901\n",
            "batch num : 738, loss:0.011443841271102428\n",
            "batch num : 739, loss:0.01153784804046154\n",
            "batch num : 740, loss:0.011268683709204197\n",
            "batch num : 741, loss:0.012450396083295345\n",
            "batch num : 742, loss:0.011538634076714516\n",
            "batch num : 743, loss:0.012149532325565815\n",
            "batch num : 744, loss:0.011545765213668346\n",
            "batch num : 745, loss:0.011262251995503902\n",
            "batch num : 746, loss:0.012128666043281555\n",
            "batch num : 747, loss:0.012999266386032104\n",
            "batch num : 748, loss:0.01194292213767767\n",
            "batch num : 749, loss:0.011497031897306442\n",
            "batch num : 750, loss:0.012900429777801037\n",
            "batch num : 751, loss:0.011016777716577053\n",
            "batch num : 752, loss:0.011391961015760899\n",
            "batch num : 753, loss:0.013315514661371708\n",
            "batch num : 754, loss:0.011743299663066864\n",
            "batch num : 755, loss:0.01171984151005745\n",
            "batch num : 756, loss:0.011793646030128002\n",
            "batch num : 757, loss:0.011876147240400314\n",
            "batch num : 758, loss:0.013167915865778923\n",
            "batch num : 759, loss:0.011834894306957722\n",
            "batch num : 760, loss:0.011806603521108627\n",
            "batch num : 761, loss:0.011944342404603958\n",
            "batch num : 762, loss:0.010363769717514515\n",
            "batch num : 763, loss:0.012846225872635841\n",
            "batch num : 764, loss:0.011610140092670918\n",
            "batch num : 765, loss:0.01188569888472557\n",
            "batch num : 766, loss:0.01295324508100748\n",
            "batch num : 767, loss:0.010824437253177166\n",
            "batch num : 768, loss:0.01102196704596281\n",
            "batch num : 769, loss:0.010967282578349113\n",
            "batch num : 770, loss:0.01121379155665636\n",
            "batch num : 771, loss:0.011180306784808636\n",
            "batch num : 772, loss:0.01349392905831337\n",
            "batch num : 773, loss:0.010581642389297485\n",
            "batch num : 774, loss:0.0109949866309762\n",
            "batch num : 775, loss:0.010730413720011711\n",
            "batch num : 776, loss:0.011350810527801514\n",
            "batch num : 777, loss:0.010434908792376518\n",
            "batch num : 778, loss:0.012156267650425434\n",
            "batch num : 779, loss:0.010453034192323685\n",
            "batch num : 780, loss:0.01162604708224535\n",
            "batch num : 781, loss:0.011995322071015835\n",
            "batch num : 782, loss:0.011892015114426613\n",
            "batch num : 783, loss:0.009877034462988377\n",
            "batch num : 784, loss:0.011701089330017567\n",
            "batch num : 785, loss:0.011455163359642029\n",
            "batch num : 786, loss:0.0111265042796731\n",
            "batch num : 787, loss:0.011206691153347492\n",
            "batch num : 788, loss:0.012567736208438873\n",
            "batch num : 789, loss:0.011953026056289673\n",
            "batch num : 790, loss:0.011030840687453747\n",
            "batch num : 791, loss:0.010899761691689491\n",
            "batch num : 792, loss:0.01182706281542778\n",
            "batch num : 793, loss:0.01108061708509922\n",
            "batch num : 794, loss:0.013500021770596504\n",
            "batch num : 795, loss:0.012297658249735832\n",
            "batch num : 796, loss:0.012475620955228806\n",
            "batch num : 797, loss:0.011158767156302929\n",
            "batch num : 798, loss:0.010868924669921398\n",
            "batch num : 799, loss:0.011513051576912403\n",
            "batch num : 800, loss:0.010690157301723957\n",
            "batch num : 801, loss:0.012885370291769505\n",
            "batch num : 802, loss:0.010599115863442421\n",
            "batch num : 803, loss:0.012222498655319214\n",
            "batch num : 804, loss:0.011265086941421032\n",
            "batch num : 805, loss:0.011228110641241074\n",
            "batch num : 806, loss:0.012683916836977005\n",
            "batch num : 807, loss:0.011556854471564293\n",
            "batch num : 808, loss:0.011738134548068047\n",
            "batch num : 809, loss:0.011194059625267982\n",
            "batch num : 810, loss:0.011392530985176563\n",
            "batch num : 811, loss:0.012007268145680428\n",
            "batch num : 812, loss:0.011100889183580875\n",
            "batch num : 813, loss:0.011995366774499416\n",
            "batch num : 814, loss:0.011659486219286919\n",
            "batch num : 815, loss:0.011163023300468922\n",
            "batch num : 816, loss:0.012075628153979778\n",
            "batch num : 817, loss:0.011485115624964237\n",
            "batch num : 818, loss:0.011775568127632141\n",
            "batch num : 819, loss:0.010682899504899979\n",
            "batch num : 820, loss:0.010712460614740849\n",
            "batch num : 821, loss:0.012465624138712883\n",
            "batch num : 822, loss:0.013340943492949009\n",
            "batch num : 823, loss:0.010585052892565727\n",
            "batch num : 824, loss:0.010265839286148548\n",
            "batch num : 825, loss:0.011382065713405609\n",
            "batch num : 826, loss:0.011032728478312492\n",
            "batch num : 827, loss:0.011815258301794529\n",
            "batch num : 828, loss:0.010630861856043339\n",
            "batch num : 829, loss:0.010516205802559853\n",
            "batch num : 830, loss:0.012049589306116104\n",
            "batch num : 831, loss:0.011354195885360241\n",
            "batch num : 832, loss:0.01067942101508379\n",
            "batch num : 833, loss:0.010998754762113094\n",
            "batch num : 834, loss:0.011556364595890045\n",
            "batch num : 835, loss:0.009908195585012436\n",
            "batch num : 836, loss:0.010668433271348476\n",
            "batch num : 837, loss:0.010979986749589443\n",
            "batch num : 838, loss:0.00996397715061903\n",
            "batch num : 839, loss:0.011788574047386646\n",
            "batch num : 840, loss:0.010421042330563068\n",
            "batch num : 841, loss:0.010052794590592384\n",
            "batch num : 842, loss:0.011159525252878666\n",
            "batch num : 843, loss:0.011541754007339478\n",
            "batch num : 844, loss:0.010810553096234798\n",
            "batch num : 845, loss:0.01150108128786087\n",
            "batch num : 846, loss:0.009418345987796783\n",
            "batch num : 847, loss:0.010766607709228992\n",
            "batch num : 848, loss:0.010743433609604836\n",
            "batch num : 849, loss:0.011350257322192192\n",
            "batch num : 850, loss:0.011543873697519302\n",
            "batch num : 851, loss:0.010096738114953041\n",
            "batch num : 852, loss:0.011083011515438557\n",
            "batch num : 853, loss:0.00959168653935194\n",
            "batch num : 854, loss:0.011371730826795101\n",
            "batch num : 855, loss:0.009574543684720993\n",
            "batch num : 856, loss:0.011600040830671787\n",
            "batch num : 857, loss:0.01049390435218811\n",
            "batch num : 858, loss:0.01089792512357235\n",
            "batch num : 859, loss:0.011989550665020943\n",
            "batch num : 860, loss:0.010928384028375149\n",
            "batch num : 861, loss:0.011621295474469662\n",
            "batch num : 862, loss:0.009849372319877148\n",
            "batch num : 863, loss:0.01248505525290966\n",
            "batch num : 864, loss:0.011402457021176815\n",
            "batch num : 865, loss:0.013007553294301033\n",
            "batch num : 866, loss:0.01212254911661148\n",
            "batch num : 867, loss:0.010121559724211693\n",
            "batch num : 868, loss:0.009680223651230335\n",
            "batch num : 869, loss:0.008864945732057095\n",
            "batch num : 870, loss:0.011145034804940224\n",
            "batch num : 871, loss:0.010285437107086182\n",
            "batch num : 872, loss:0.011778014712035656\n",
            "batch num : 873, loss:0.010836341418325901\n",
            "batch num : 874, loss:0.0124594671651721\n",
            "batch num : 875, loss:0.009820222854614258\n",
            "batch num : 876, loss:0.011351369321346283\n",
            "batch num : 877, loss:0.010656261816620827\n",
            "batch num : 878, loss:0.010456149466335773\n",
            "batch num : 879, loss:0.01039984729140997\n",
            "batch num : 880, loss:0.00977938063442707\n",
            "batch num : 881, loss:0.011565704829990864\n",
            "batch num : 882, loss:0.010415944270789623\n",
            "batch num : 883, loss:0.010723084211349487\n",
            "batch num : 884, loss:0.011190418154001236\n",
            "batch num : 885, loss:0.010305274277925491\n",
            "batch num : 886, loss:0.01070492435246706\n",
            "batch num : 887, loss:0.01116881426423788\n",
            "batch num : 888, loss:0.010198880918323994\n",
            "batch num : 889, loss:0.009780278429389\n",
            "batch num : 890, loss:0.010410768911242485\n",
            "batch num : 891, loss:0.010761458426713943\n",
            "batch num : 892, loss:0.010006080381572247\n",
            "batch num : 893, loss:0.010316104628145695\n",
            "batch num : 894, loss:0.011268273927271366\n",
            "batch num : 895, loss:0.010006530210375786\n",
            "batch num : 896, loss:0.00901828147470951\n",
            "batch num : 897, loss:0.01221691444516182\n",
            "batch num : 898, loss:0.00972224771976471\n",
            "batch num : 899, loss:0.009810521267354488\n",
            "batch num : 900, loss:0.011188056319952011\n",
            "batch num : 901, loss:0.009274648502469063\n",
            "batch num : 902, loss:0.009095723740756512\n",
            "batch num : 903, loss:0.011084062047302723\n",
            "batch num : 904, loss:0.0104175740852952\n",
            "batch num : 905, loss:0.010251619853079319\n",
            "batch num : 906, loss:0.010680627077817917\n",
            "batch num : 907, loss:0.009238826110959053\n",
            "batch num : 908, loss:0.01054987870156765\n",
            "batch num : 909, loss:0.00906432792544365\n",
            "batch num : 910, loss:0.008893674239516258\n",
            "batch num : 911, loss:0.010082880035042763\n",
            "batch num : 912, loss:0.011064081452786922\n",
            "batch num : 913, loss:0.010820737108588219\n",
            "batch num : 914, loss:0.009417727589607239\n",
            "batch num : 915, loss:0.009450516663491726\n",
            "batch num : 916, loss:0.00987233780324459\n",
            "batch num : 917, loss:0.009678641334176064\n",
            "batch num : 918, loss:0.012334650382399559\n",
            "batch num : 919, loss:0.01114432979375124\n",
            "batch num : 920, loss:0.01049727201461792\n",
            "batch num : 921, loss:0.011289997957646847\n",
            "batch num : 922, loss:0.009507190436124802\n",
            "batch num : 923, loss:0.008411136455833912\n",
            "batch num : 924, loss:0.009116912260651588\n",
            "batch num : 925, loss:0.010106979869306087\n",
            "batch num : 926, loss:0.009942783042788506\n",
            "batch num : 927, loss:0.00980432704091072\n",
            "batch num : 928, loss:0.01116191130131483\n",
            "batch num : 929, loss:0.009875248186290264\n",
            "batch num : 930, loss:0.010658985003829002\n",
            "batch num : 931, loss:0.010398780927062035\n",
            "batch num : 932, loss:0.009835561737418175\n",
            "batch num : 933, loss:0.010719116777181625\n",
            "batch num : 934, loss:0.009667129255831242\n",
            "batch num : 935, loss:0.00965921301394701\n",
            "batch num : 936, loss:0.0105852410197258\n",
            "batch num : 937, loss:0.010319582186639309\n",
            "batch num : 938, loss:0.007927979342639446\n",
            "batch num : 939, loss:0.009530561044812202\n",
            "batch num : 940, loss:0.00965887401252985\n",
            "batch num : 941, loss:0.010731173679232597\n",
            "batch num : 942, loss:0.010355174541473389\n",
            "batch num : 943, loss:0.00931533146649599\n",
            "batch num : 944, loss:0.009511473588645458\n",
            "batch num : 945, loss:0.010783306322991848\n",
            "batch num : 946, loss:0.010106462985277176\n",
            "batch num : 947, loss:0.010448059067130089\n",
            "batch num : 948, loss:0.009933248162269592\n",
            "batch num : 949, loss:0.010180790908634663\n",
            "batch num : 950, loss:0.008555153384804726\n",
            "batch num : 951, loss:0.009471596218645573\n",
            "batch num : 952, loss:0.0092981718480587\n",
            "batch num : 953, loss:0.009916627779603004\n",
            "batch num : 954, loss:0.010397886857390404\n",
            "batch num : 955, loss:0.01159665361046791\n",
            "batch num : 956, loss:0.009466211311519146\n",
            "batch num : 957, loss:0.01065430324524641\n",
            "batch num : 958, loss:0.010105234570801258\n",
            "batch num : 959, loss:0.011908156797289848\n",
            "batch num : 960, loss:0.010458149947226048\n",
            "batch num : 961, loss:0.009983508847653866\n",
            "batch num : 962, loss:0.010541540570557117\n",
            "batch num : 963, loss:0.009267699904739857\n",
            "batch num : 964, loss:0.011274500750005245\n",
            "batch num : 965, loss:0.00919538363814354\n",
            "batch num : 966, loss:0.010613645426928997\n",
            "batch num : 967, loss:0.01022277306765318\n",
            "batch num : 968, loss:0.009162552654743195\n",
            "batch num : 969, loss:0.01007301826030016\n",
            "batch num : 970, loss:0.011076108552515507\n",
            "batch num : 971, loss:0.0110343461856246\n",
            "batch num : 972, loss:0.01070317905396223\n",
            "batch num : 973, loss:0.01052728109061718\n",
            "batch num : 974, loss:0.008917244151234627\n",
            "batch num : 975, loss:0.010300560854375362\n",
            "batch num : 976, loss:0.010931681841611862\n",
            "batch num : 977, loss:0.009309946559369564\n",
            "batch num : 978, loss:0.011308661662042141\n",
            "batch num : 979, loss:0.008875912055373192\n",
            "batch num : 980, loss:0.009851424023509026\n",
            "batch num : 981, loss:0.013108126819133759\n",
            "batch num : 982, loss:0.009416799992322922\n",
            "batch num : 983, loss:0.009640193544328213\n",
            "batch num : 984, loss:0.009128016419708729\n",
            "batch num : 985, loss:0.00966411642730236\n",
            "batch num : 986, loss:0.009822598658502102\n",
            "batch num : 987, loss:0.010875166393816471\n",
            "batch num : 988, loss:0.010851496830582619\n",
            "batch num : 989, loss:0.009299911558628082\n",
            "batch num : 990, loss:0.009949513711035252\n",
            "batch num : 991, loss:0.009506098926067352\n",
            "batch num : 992, loss:0.010499823838472366\n",
            "batch num : 993, loss:0.010298481211066246\n",
            "batch num : 994, loss:0.01033930853009224\n",
            "batch num : 995, loss:0.010029767639935017\n",
            "batch num : 996, loss:0.010186516679823399\n",
            "batch num : 997, loss:0.008998568169772625\n",
            "batch num : 998, loss:0.009011267684400082\n",
            "batch num : 999, loss:0.009036430157721043\n",
            "batch num : 1000, loss:0.009612013585865498\n",
            "batch num : 1001, loss:0.008725283667445183\n",
            "batch num : 1002, loss:0.009517024271190166\n",
            "batch num : 1003, loss:0.008600493893027306\n",
            "batch num : 1004, loss:0.010745324194431305\n",
            "batch num : 1005, loss:0.009376117959618568\n",
            "batch num : 1006, loss:0.009117058478295803\n",
            "batch num : 1007, loss:0.010329796932637691\n",
            "batch num : 1008, loss:0.009885329753160477\n",
            "batch num : 1009, loss:0.011058521457016468\n",
            "batch num : 1010, loss:0.012529512867331505\n",
            "batch num : 1011, loss:0.00889615248888731\n",
            "batch num : 1012, loss:0.010090477764606476\n",
            "batch num : 1013, loss:0.00864084716886282\n",
            "batch num : 1014, loss:0.009162146598100662\n",
            "batch num : 1015, loss:0.008537156507372856\n",
            "batch num : 1016, loss:0.010549421422183514\n",
            "batch num : 1017, loss:0.0090224239975214\n",
            "batch num : 1018, loss:0.009090602397918701\n",
            "batch num : 1019, loss:0.009091900661587715\n",
            "batch num : 1020, loss:0.010974698700010777\n",
            "batch num : 1021, loss:0.009398934431374073\n",
            "batch num : 1022, loss:0.008956597186625004\n",
            "batch num : 1023, loss:0.010538902133703232\n",
            "batch num : 1024, loss:0.009660271927714348\n",
            "batch num : 1025, loss:0.009460540488362312\n",
            "batch num : 1026, loss:0.009757550433278084\n",
            "batch num : 1027, loss:0.009387679398059845\n",
            "batch num : 1028, loss:0.00859983079135418\n",
            "batch num : 1029, loss:0.009664159268140793\n",
            "batch num : 1030, loss:0.011102442629635334\n",
            "batch num : 1031, loss:0.009285402484238148\n",
            "batch num : 1032, loss:0.009078122675418854\n",
            "batch num : 1033, loss:0.010295608080923557\n",
            "batch num : 1034, loss:0.01061323843896389\n",
            "batch num : 1035, loss:0.009550794027745724\n",
            "batch num : 1036, loss:0.010492227040231228\n",
            "batch num : 1037, loss:0.00891781784594059\n",
            "batch num : 1038, loss:0.010197577998042107\n",
            "batch num : 1039, loss:0.009906441904604435\n",
            "batch num : 1040, loss:0.00955035537481308\n",
            "batch num : 1041, loss:0.009251538664102554\n",
            "batch num : 1042, loss:0.009285260923206806\n",
            "batch num : 1043, loss:0.009514203295111656\n",
            "batch num : 1044, loss:0.009583769366145134\n",
            "batch num : 1045, loss:0.011650139465928078\n",
            "batch num : 1046, loss:0.008774097077548504\n",
            "batch num : 1047, loss:0.009715643711388111\n",
            "batch num : 1048, loss:0.009172276593744755\n",
            "batch num : 1049, loss:0.008625788614153862\n",
            "batch num : 1050, loss:0.009007244370877743\n",
            "batch num : 1051, loss:0.009581400081515312\n",
            "batch num : 1052, loss:0.00878316443413496\n",
            "batch num : 1053, loss:0.009286431595683098\n",
            "batch num : 1054, loss:0.008852978236973286\n",
            "batch num : 1055, loss:0.010796518065035343\n",
            "batch num : 1056, loss:0.010494738817214966\n",
            "batch num : 1057, loss:0.008084922097623348\n",
            "batch num : 1058, loss:0.007815239951014519\n",
            "batch num : 1059, loss:0.010475216433405876\n",
            "batch num : 1060, loss:0.010417213663458824\n",
            "batch num : 1061, loss:0.009637237526476383\n",
            "batch num : 1062, loss:0.009676524437963963\n",
            "batch num : 1063, loss:0.00822092778980732\n",
            "batch num : 1064, loss:0.009216804057359695\n",
            "batch num : 1065, loss:0.010494227521121502\n",
            "batch num : 1066, loss:0.00850249920040369\n",
            "batch num : 1067, loss:0.00911741703748703\n",
            "batch num : 1068, loss:0.008736914955079556\n",
            "batch num : 1069, loss:0.008339731954038143\n",
            "batch num : 1070, loss:0.009798050858080387\n",
            "batch num : 1071, loss:0.009173791855573654\n",
            "batch num : 1072, loss:0.011201086454093456\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ef1a4a50c5fd>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"batch num : {batch_idx}, loss:{loss.item()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model.to('cpu')\n",
        "image, mask = get_batch_tensor(1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(image)[\"out\"]\n",
        "image = image.squeeze().cpu().numpy()\n",
        "image = image.transpose(1, 2, 0)\n",
        "\n",
        "prediction = output.squeeze().cpu().numpy()\n",
        "prediction = np.argmax(prediction, axis=0)\n",
        "prediction = prediction * 127\n",
        "prediction = prediction.astype(np.uint8)\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "ax[0].imshow(image)\n",
        "ax[0].set_title('Random Shapes')\n",
        "ax[0].axis('off')\n",
        "\n",
        "ax[1].imshow(prediction, cmap='gray')\n",
        "ax[1].set_title('prediction for Rectangle')\n",
        "ax[1].axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "xc75Yn8MFfao",
        "outputId": "5685ed3e-7210-4f90-efc5-6c00ccd4ea00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}