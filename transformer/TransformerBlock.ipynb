{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjk39XwMWfw+lAaG6zOxYO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoCodeProgram/deepLearning/blob/main/transformer/TransformerBlock.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIEtq_VdnOTX",
        "outputId": "b9d7cbb9-0a83-439b-cb51-86efc18d2188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'deepLearning' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NoCodeProgram/deepLearning.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# Read the text file\n",
        "with open('deepLearning/transformer/shakespeare.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Tokenize the text  (this is very simple tokenizer, in reality you would use a more advanced one)\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "tokens = tokenizer(text)\n",
        "unique_tokens = set(tokens)\n"
      ],
      "metadata": {
        "id": "bO4qRfv5oTix"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = { s:i for i,s in enumerate(unique_tokens)}\n",
        "itos = { i:s for i,s in enumerate(unique_tokens)}\n",
        "# print(stoi)\n",
        "# print(itos)\n",
        "\n",
        "vocab_size = len(unique_tokens)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MHEK-7EoVaT",
        "outputId": "3d236742-7b15-4ce0-fa86-3cdf8ca14654"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"i love you all\"\n",
        "indices = [stoi[word] for word in sentence.split()]\n",
        "print(indices)\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "embedding_dim = 20\n",
        "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "embedded_sentence = embedding(torch.tensor(indices))\n",
        "print(embedded_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zN5ZwH-onR1",
        "outputId": "cac6d972-217d-4237-f6f6-eb27a16478f0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[703, 2324, 2988, 736]\n",
            "tensor([[ 0.5823,  0.2879, -0.6389,  0.5345,  0.2990, -0.5058, -0.6320,  0.7645,\n",
            "         -1.0935, -0.2994,  0.5853,  0.0207,  0.0556, -0.6536, -0.7247, -0.9703,\n",
            "          0.9994, -1.1219,  0.1807, -1.0605],\n",
            "        [ 2.0182,  1.8420, -0.9888,  1.2260, -0.1556,  0.6729,  0.0403,  0.1177,\n",
            "         -0.5989, -0.7594, -0.1346,  0.3288,  1.4488,  0.8282, -0.5927,  1.4262,\n",
            "          1.5454,  0.5162,  0.3055, -0.6019],\n",
            "        [ 0.8747,  0.4409, -0.0992,  0.8930,  0.6899,  0.3536,  0.0701,  0.2687,\n",
            "          0.6222, -0.9365,  0.3176, -0.9085, -1.2387,  0.4030, -1.1117,  0.4290,\n",
            "          1.0424,  0.1438, -1.4887, -0.2270],\n",
            "        [-0.1886, -0.2868,  0.4766, -0.9368,  0.3664,  0.3415, -0.1182, -1.2735,\n",
            "         -0.1879,  0.5929,  2.7290, -0.2996,  1.3779,  0.6140,  2.0823,  0.1850,\n",
            "          0.1959, -0.2307, -0.4035,  0.4000]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, atten_dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(embed_dim, atten_dim, bias=False)\n",
        "        self.key = nn.Linear(embed_dim, atten_dim, bias=False)\n",
        "        self.value = nn.Linear(embed_dim, atten_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        query = self.query(x)\n",
        "        key = self.key(x)\n",
        "        value = self.value(x)\n",
        "\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
        "        scores = scores / key.size(-1)**0.5\n",
        "\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        weighted_values = torch.matmul(attention_weights, value)\n",
        "\n",
        "        return weighted_values"
      ],
      "metadata": {
        "id": "FLdGgCoioV-l"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        attention_dim = embed_dim // num_heads\n",
        "        self.attentions = nn.ModuleList([SelfAttention(embed_dim, attention_dim) for _ in range(num_heads)])\n",
        "        self.fc = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        head_outputs = []\n",
        "        for attention in self.attentions:\n",
        "            head_output = attention(x)\n",
        "            head_outputs.append(head_output)\n",
        "\n",
        "        concatenated_heads = torch.cat(head_outputs, dim=-1)\n",
        "        print(\"concatenated_heads\", concatenated_heads.shape)\n",
        "        output = self.fc(concatenated_heads)\n",
        "        print(\"output\", output.shape)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "DDvckYImocdW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "jfR9B06RoelC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, n_head):\n",
        "        super().__init__()\n",
        "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.multihead_atten = MultiheadAttention(embed_dim, n_head)\n",
        "\n",
        "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.feed_forward = FeedFoward(embed_dim, 4*embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.multihead_atten(self.layer_norm1(x))\n",
        "        x = x + self.feed_forward(self.layer_norm2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "BJ9o5xF5oglr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_heads = 4\n",
        "\n",
        "output = TransformerBlock(embedding_dim, num_heads)(embedded_sentence)\n",
        "print(\"output shape\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXIjIqIGoiqP",
        "outputId": "117421a2-c8cf-48a8-ac53-ea0d904f757a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "concatenated_heads torch.Size([4, 20])\n",
            "output torch.Size([4, 20])\n",
            "output shape torch.Size([4, 20])\n"
          ]
        }
      ]
    }
  ]
}